{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reinforcement learning in 1d maze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tau(s,a):\n",
    "    if s==0 or s==4:  return(s)\n",
    "    else:      return(s+a)\n",
    "    \n",
    "def rho(s,a):\n",
    "    return (s==1 and a==-1)+2*(s==3 and a == 1)\n",
    "\n",
    "def calc_policy(Q):\n",
    "    policy=np.zeros(5)\n",
    "    for s in range(0,5):\n",
    "        action_idx=np.argmax(Q[s,:])\n",
    "        policy[s]=2*action_idx-1\n",
    "        policy[0]=policy[4]=0\n",
    "    return policy.astype(int)\n",
    "\n",
    "def idx(a):\n",
    "    return(int((a+1)/2))\n",
    "\n",
    "gamma=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> Analytic solution for optimal policy')\n",
    "# Defining reward vector R\n",
    "i=0; R=np.zeros(10)\n",
    "for s in range(0,5):\n",
    "    for a in range(-1,2,2):       \n",
    "        R[i]=rho(s,a)\n",
    "        i += 1\n",
    "        \n",
    "# Defining transition matrix\n",
    "T=np.zeros([10,10]);\n",
    "T[0,0]=1; T[1,1]=1; T[2,0]=1; T[3,5]=1; T[4,2]=1\n",
    "T[5,7]=1; T[6,5]=1; T[7,9]=1; T[8,7]=1; T[9,9]=1\n",
    "\n",
    "# Calculate Q-function\n",
    "Q=np.linalg.inv(np.eye(10)-gamma*T) @ np.transpose(R)\n",
    "Q=np.reshape(Q,[5,2]); Q[4,0]=0\n",
    "\n",
    "policy=calc_policy(Q)\n",
    "print('Q values: \\n',np.transpose(Q))\n",
    "print('policy: \\n',np.transpose(policy))\n",
    "Qana=Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> Dynamic Programming')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "for iter in range(3):\n",
    "    for s in range(0,5): \n",
    "        for a in range(-1,2,2):\n",
    "            act = np.int(policy[tau(s,a)])\n",
    "            Q[s,idx(a)]=rho(s,a)+gamma*Q[tau(s,a),idx(act)]\n",
    "\n",
    "print('Q values: \\n',np.transpose(Q))\n",
    "print('policy: \\n',np.transpose(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> Policy iteration')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "policy=calc_policy(Q)\n",
    "for iter in range(3):\n",
    "    for s in range(0,5): \n",
    "        for a in range(-1,2,2):\n",
    "            act = np.int(policy[tau(s,a)])\n",
    "            Q[s,idx(a)]=rho(s,a)+gamma*Q[tau(s,a),idx(act)]\n",
    "    policy=calc_policy(Q)\n",
    "    \n",
    "print('Q values: \\n',np.transpose(Q))\n",
    "print('policy: \\n',np.transpose(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "print('--> Q-iteration')\n",
    "\n",
    "Q_new=np.zeros([5,2])\n",
    "Q=np.zeros([5,2])\n",
    "for iter in range(3):\n",
    "    for s in range(0,5): \n",
    "        for a in range(-1,2,2):\n",
    "            maxValue = np.maximum(Q[tau(s,a),0],Q[tau(s,a),1])\n",
    "            Q_new[s,idx(a)]=rho(s,a)+gamma*maxValue\n",
    "    Q=np.copy(Q_new);\n",
    " \n",
    "policy=calc_policy(Q)\n",
    "print('Q values: \\n',np.transpose(Q))\n",
    "print('policy: \\n',np.transpose(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> SARSA')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "error = []\n",
    "alpha=1;\n",
    "for trial in range(200):\n",
    "    policy=calc_policy(Q)\n",
    "    s=2\n",
    "    for t in range(0,5):\n",
    "        a=policy[s]\n",
    "        if np.random.rand()<0.1: a=-a #epsilon greedy\n",
    "        a2=idx(policy[tau(s,a)])\n",
    "        TD=rho(s,a)+gamma*Q[tau(s,a),a2]-Q[s,idx(a)]\n",
    "        Q[s,idx(a)]=Q[s,idx(a)]+alpha*TD\n",
    "        s=tau(s,a)\n",
    "    error.append(np.sum(np.sum(np.abs(np.subtract(Q,Qana))))) \n",
    "\n",
    "print('Q values: \\n',np.transpose(Q))\n",
    "print('policy: \\n',np.transpose(policy))\n",
    "plt.figure(); plt.plot(error)\n",
    "plt.xlabel('iteration'); plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> Q-Learning:')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "alpha=1\n",
    "error = []\n",
    "for trial in range(200):\n",
    "    policy=calc_policy(Q)\n",
    "    s=2\n",
    "    for t in range(0,5):\n",
    "        a=policy[s]\n",
    "        if np.random.rand()<0.2: a=-a #epsilon greedy\n",
    "        TD=rho(s,a)+gamma*np.maximum(Q[tau(s,a),0],Q[tau(s,a),1])-Q[s,idx(a)]\n",
    "        Q[s,idx(a)]=Q[s,idx(a)]+alpha*TD\n",
    "        Q[0]=0;Q[4]=0;  \n",
    "        s=tau(s,a)\n",
    "    error.append(np.sum(np.sum(np.abs(np.subtract(Q,Qana))))) \n",
    "\n",
    "print('Q values: \\n',np.transpose(Q))\n",
    "print('policy: \\n',np.transpose(policy))\n",
    "plt.plot(error,'r'); \n",
    "plt.xlabel('iteration'); plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> TD(lambda) for Q-learning')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "alpha=1\n",
    "error = []\n",
    "eligibility=np.zeros([5,2])\n",
    "lam=0.7\n",
    "\n",
    "for trial in range(200):\n",
    "    policy=calc_policy(Q)\n",
    "    s=2\n",
    "    for t in range(0,5):\n",
    "        a=policy[s]\n",
    "        if np.random.rand()<0.1: a=-a #epsilon greedy\n",
    "        TD=rho(s,a)+gamma*np.maximum(Q[tau(s,a),0],Q[tau(s,a),1])-Q[s,idx(a)]\n",
    "        eligibility*=gamma*lam\n",
    "        eligibility[s,idx(a)]=1\n",
    "        for si in range(1,4):\n",
    "            for ai in range(2): \n",
    "                Q[si,ai]=Q[si,ai]+alpha*TD*eligibility[si,ai]\n",
    "        Q[0]=0;Q[4]=0;  \n",
    "        s=tau(s,a)\n",
    "    error.append(np.sum(np.sum(np.abs(np.subtract(Q,Qana))))) \n",
    "\n",
    "print('Q values: \\n',np.transpose(Q))\n",
    "print('policy: \\n',np.transpose(policy))\n",
    "plt.plot(error,'r'); \n",
    "plt.xlabel('iteration'); plt.ylabel('error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
